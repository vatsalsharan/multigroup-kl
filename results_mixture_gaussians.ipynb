{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiA as ma\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from Node import BranchingProgram\n",
    "from Node import Node\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 30\n",
    "n_cells = np.zeros(2*L)\n",
    "n_cells[L] = 1\n",
    "lam = 1.3\n",
    "for i in np.arange(1,L):\n",
    "    n_cells[L + i] = n_cells[L+i-1] * lam\n",
    "    n_cells[L - i] = n_cells[L - i + 1] / lam\n",
    "print(n_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_kl(mu0,mu1,cov0,cov1):\n",
    "    cov1inv = np.linalg.inv(cov1)\n",
    "    det1 = np.linalg.det(cov1)\n",
    "    det0 = np.linalg.det(cov0)\n",
    "    x = np.trace(np.matmul(cov1inv,cov0)) + np.matmul(np.matmul(np.transpose(mu1 - mu0),cov1inv), (mu1-mu0)) - len(mu0) + np.log(det1/det0)\n",
    "    x = x/(2*np.log(2))\n",
    "    return x\n",
    "\n",
    "def return_p_sample(N, k, means_p, sigma):\n",
    "    per_comp = int(N/k)\n",
    "    data_p = np.random.multivariate_normal(means_p[0,:], sigma, per_comp)\n",
    "    for i in np.arange(1,k):\n",
    "        add_data = np.random.multivariate_normal(means_p[i,:], sigma, per_comp)\n",
    "        data_p = np.concatenate((data_p,add_data), axis = 0)\n",
    "    data_p = np.random.permutation(data_p)\n",
    "    return data_p\n",
    "\n",
    "def return_r_sample(N, k, means_r, sigma):\n",
    "    per_comp = int(N/k)\n",
    "    data_r = np.random.multivariate_normal(means_r[0,:], sigma, per_comp)\n",
    "    for i in np.arange(1,k):\n",
    "        add_data = np.random.multivariate_normal(means_r[i,:], sigma, per_comp)\n",
    "        data_r = np.concatenate((data_r,add_data), axis = 0)\n",
    "    data_r = np.random.permutation(data_r)\n",
    "    return data_r\n",
    "\n",
    "def return_p_sample_label(N, k, means_p, sigma):\n",
    "    per_comp = int(N/k)\n",
    "    data_p = np.random.multivariate_normal(means_p[0,:], sigma, per_comp)\n",
    "    y = np.zeros(per_comp)\n",
    "    for i in np.arange(1,k):\n",
    "        add_data = np.random.multivariate_normal(means_p[i,:], sigma, per_comp)\n",
    "        add_y = (2*i)*np.ones(per_comp)\n",
    "        data_p = np.concatenate((data_p,add_data), axis = 0)\n",
    "        y = np.concatenate((y,add_y), axis = 0)\n",
    "    return data_p, y\n",
    "\n",
    "def return_r_sample_label(N, k, means_r, sigma):\n",
    "    per_comp = int(N/k)\n",
    "    data_r = np.random.multivariate_normal(means_r[0,:], sigma, per_comp)\n",
    "    y = np.ones(per_comp)\n",
    "    for i in np.arange(1,k):\n",
    "        add_data = np.random.multivariate_normal(means_r[i,:], sigma, per_comp)\n",
    "        add_y = (2*i+1)*np.ones(per_comp)\n",
    "        data_r = np.concatenate((data_r,add_data), axis = 0)\n",
    "        y = np.concatenate((y,add_y), axis = 0)\n",
    "    return data_r, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [5,10,15]\n",
    "repeats = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    return RandomForestClassifier(max_depth=5, n_estimators=10)\n",
    "#     return LogisticRegression(max_iter=500, C=1)\n",
    "\n",
    "for i,k in enumerate(K):\n",
    "    mc_kl = np.zeros(repeats)\n",
    "    me_kl = np.zeros(repeats)\n",
    "    mc_kl_std = np.zeros(repeats)\n",
    "    me_kl_std = np.zeros(repeats)\n",
    "    for count in range(repeats):\n",
    "        d = 2\n",
    "        delta = 2.5*np.ones(d)\n",
    "\n",
    "        sigma = np.eye(d)\n",
    "        means_p = np.random.multivariate_normal(np.zeros(d), k*10000*sigma, k)\n",
    "        means_r = means_p + delta\n",
    "        kl_approx = 10000*np.ones((k,k))\n",
    "        for ii in range(k):\n",
    "            for jj in np.arange(ii+1,k):\n",
    "                kl_approx[ii,jj] = normal_kl(means_r[ii,:],means_p[jj,:],sigma,sigma) \n",
    "        print(kl_approx.min())\n",
    "\n",
    "        true_kl = normal_kl(means_r[1,:],means_p[1,:],sigma,sigma) \n",
    "        print(true_kl)\n",
    "\n",
    "        N = 500000\n",
    "        data_p = return_p_sample(N, k, means_p, sigma)\n",
    "        data_r = return_r_sample(N, k, means_r, sigma)\n",
    "\n",
    "        A = BranchingProgram(n_cells, 25, 0.02, classifier, reweigh = 0)\n",
    "        A.fit(data_p,data_r)\n",
    "        A.compute_kl()\n",
    "\n",
    "        data_p = return_p_sample(N, k, means_p, sigma)\n",
    "        data_r = return_r_sample(N, k, means_r, sigma)\n",
    "\n",
    "        mc_kl[count] = A.predict_kl_pair(data_p, data_r)\n",
    "\n",
    "        data_p = return_p_sample(N, k, means_p, sigma)\n",
    "        data_r = return_r_sample(N, k, means_r, sigma)\n",
    "\n",
    "        me = ma.MaxEnt()\n",
    "        me.fit(data_p, data_r, iter=1000, clr_maker=classifier, eps=0.02, eta=0.02)\n",
    "\n",
    "        data_p = return_p_sample(N, k, means_p, sigma)\n",
    "        data_r = return_r_sample(N, k, means_r, sigma)\n",
    "\n",
    "        me_kl[count] = me.compute_KL(data_r)\n",
    "\n",
    "        mc_subgroup = np.zeros(k)\n",
    "        me_subgroup = np.zeros(k)\n",
    "        for ii in range(k):\n",
    "            per_comp = 100000\n",
    "            data_p = np.random.multivariate_normal(means_p[ii,:], sigma, per_comp)\n",
    "            data_r = np.random.multivariate_normal(means_r[ii,:], sigma, per_comp)\n",
    "            mc_subgroup[ii] = A.predict_kl_pair(data_p, data_r)\n",
    "            me_subgroup[ii] = me.compute_KL(data_r)\n",
    "            \n",
    "        mc_kl_std[count] = np.std(mc_subgroup)\n",
    "        me_kl_std[count] = np.std(me_subgroup)\n",
    "\n",
    "    mdic = {\"N\": N, \"means_p\": means_p, \"means_r\": means_r, \"mc_subgroup\":mc_subgroup, \"me_subgroup\":me_subgroup, \"delta\":delta, \"me_kl\":me_kl,\n",
    "       \"mc_kl\":mc_kl,\"d\": d, \"k\": k, \"true_kl\": true_kl,\"me_kl\": me_kl, \"mc_kl\": mc_kl,\n",
    "           \"me_kl_std\": me_kl_std, \"mc_kl_std\": mc_kl_std}\n",
    "    name = 'gaussian_d'+str(d)+'_k'+str(k)+'.mat'\n",
    "    scipy.io.savemat(name, mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
